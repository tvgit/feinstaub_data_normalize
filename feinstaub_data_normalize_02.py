# -*- coding: utf-8 -*-
# 2017_07_27-23_58_16 generated by: >pyprogen.py<
# >64398a27f4d8648745ede4bf33a8a54b< 

# YOUR code resides in THIS module.

"""
Vs 02


Dieses schöne Programm:
liest Daten aus einer sqlite-db, 
modifiziert sie und 
schreibt sie in eine andere sqlite-db:
(db_in)  db_fn  = r"\\RB3-WORK\lighttpd\feinstaub_0011.db"
(db_out) db_out = r'C:\tmp\sqlite\feinstaub_0011_NORM.db'

Modifizieren geschieht in: >def normalize_and_save_data(...):< und heißt:
 - die Messzeitpunkte zu 'normalisieren' == um ca 150 sec nach früher/später verscehieben, 
   damit sie auf einem vorgegebenen Gitter von Zeitpunkten zu liegen kommen (um besser vergleichbar zu sein)
 - Wenn Lücken auftreten, beim Beginn sqlite: Null (python: None) einfügen. Dadurch werden Lücken in den 
   Zeitreihen der Datensätze in plotly nicht verbunden (sondern bleiben leer)

2017-09-29
 - Neu: die Messwerte kommen in eine weitere Tabelle, jeweils: Zeitpunkt - (Messstation - GPS - Sensor) - Wert.
   https://dba.stackexchange.com/questions/129023/selecting-data-from-another-table-using-a-foreign-key
   Die Werte "Messstation - GPS - Sensor" können in eine eigene Tabelle ausgelagert werden und mittels JOIN
   wieder im Ergebnis auftauchen.
   http://sql.lernenhoch2.de/lernen/sql-fortgeschritten/join-tabelle-zusammenfugen/

   nb: esp8266id == last 24 Bit of MAC address (first 24 Bit = manufacturer)
"""


# TODO
# - Zusammenführung von
#     feinstaub_data_to_database.py    und
#     feinstaub_data_normalize_02.py
# - Relative Pfade in beiden Programmen
# - eine gemeinsame cfg-Datei ...

import lib.x_CAParser as x_CAParser
import lib.ppg_utils  as p_utils
from   lib.ppg_log    import p_log_init, p_log_start, p_log_this, p_log_end
import lib.x_glbls
import lib.fstb_dta_to_db_mod as fstb_dta_to_db_mod

import os

import sqlite3
import json
import pandas as pd

from StringIO import StringIO
import prettytable

import time
import sys
import datetime

# 'confargs' are your configuration parameters / cmdline arguments
confargs = lib.x_glbls.arg_ns

cnt_files = 0
cnt_lines = 0

cnt_files_fail = 0
cnt_lines_fail = 0

delta_t_mess = 150  # Normierte Zeitspanne zwischen zwei Messungen desselben Sensors

# INPUT database:
# originale Messwerte: utime's sind nicht in 150 sec Raster
# db_in = r'feinstaub_0011.db'
# db_in = r"\\rb3-work\lighttpd\feinstaub_0011.db"
# db_in = r"\\RB3-WORK\lighttpd\feinstaub_0011_JSON.db"

class mem():
    # class that stores global vars (not a real good idea: abuse of class construct:
    #    mem acts as kind of container of global accessible variables ... )
    # INPUT database == db_in

    # db_in  == path of db with JSON data        ... is initialised in >main()< !! ugly
    db_in = ''
    # db_out == path of db with normalised data  ... is initialised in >main()< !! ugly
    db_out = ''

    # INPUT database tables in db_in: db_in_table_fstb, ....
    db_in_table_fstb = 'fstb'
    db_in_table_fstb_JSON = 'fstb_JSON'  # db_in !

    # OUTPUT database:
    # 1. Es gibt eine Tabelle mit identischer Tabellenstruktur, wie die der Ursprungstabelle,
    #    aber die utime's der Messwerte sind in 150 sec Raster.
    # 2. und eine andere Tabelle, eine quasi normalisierte, in der pro Zeile nur ein Messwert auftaucht
    #    (und nicht die Messwerte aller Sensoren). Dazu kommt eine weitere, eine Untertabelle, in der die
    #    Spezifika des dazugehörigen Sensors abgelegt sind: Messstation, Typ, Name ...
    # db_out = r'feinstaub_0011_NORM.db'

    db_out_table_fstb             = 'fstb'       # in
    db_out_table_fstb_JSON        = 'fstb_JSON'  # out
    db_out_table_fstb_pivot       = 'fstb_pivot'
    db_out_table_values           = '"values"'   # >values< ist eigentlich keyword in sqlite. Aber in >"< dann doch Identifier.
    db_out_table_sensors          = 'sensors'
    db_out_table_stations         = 'stations'
    db_out_table_stations_sensors = 'stations_sensors' # Für alle Mess-Stationen jewwils alle Sensoren
    db_out_table_day_station      = 'day_stations'     # Tage, an denen es Werte für die Mess-Station >station_name< gibt.


# https://pymotw.com/2/sqlite3/
def db_table_get_column_names(db_name, table_name):
    # http://sebastianraschka.com/Articles/2014_sqlite_in_python_tutorial.html#retrieving-column-names
    # Sebastian Raschka, 2014
    # Getting column names of an SQLite database table
    conn = sqlite3.connect(db_name)
    c = conn.cursor()
    # Retrieve column information
    # Every column will be represented by a tuple with the following attributes:
    # (id, name, type, notnull, default_value, primary_key)
    c.execute('PRAGMA TABLE_INFO({})'.format(table_name))
    # collect names in a list
    names = [tup[1] for tup in c.fetchall()]
    # print(table_name, names)
    conn.close()
    return names


def db_out_make(db_out):
    if not os.path.exists(db_out) :
        msge = 'database: >' + db_out + '< does not exist.'
        print "db_out_make: " + msge; p_log_this(msge)
        msge = 'db_out_make: try '
        print msge; p_log_this(msge)
        try:
            conn = sqlite3.connect(db_out)
            msge = 'db_out_make: database: >' + db_out + '< made.'
            print msge; p_log_this(msge)
        # except Error as e:
        except:
            msge = 'db_out_make: Error making database: >' + db_out + '< .'
            p_utils.p_terminal_mssge_error(msge);  p_log_this(msge)
            # msge = Error  ; print(Error)
        finally:
            conn.close()
            pass
    else:
        msge = 'database: >' + db_out + '< created.'
        print msge; p_log_this(msge)


def table_create(db_name, table_name, sql):
#   mssge_SQL_failed(loc=loc, db_out='', sql=sql, row=row)
    msges = []
    msge  = 'table_create: database = >' + db_name + '<   table: >' + table_name + '< : '

    try:
        with sqlite3.connect(db_name) as conn:
            conn.executescript(sql)
        msges.append(msge + ' ok!')
    except:
        msges.append(msge)
        msges.append('sql: >' + sql + '<')
        msges.append('ERROR! ')
        msg = str(db_name) + ': ' + str(table_name) +  '  ' + str(sql)
        p_utils.p_terminal_mssge_error(msg); p_log_this(msg)
        p_log_this(msg)

    for msge in msges:
        p_log_this(msge); print msge


def db_out_tables_create(db_out):

    table_name = mem.db_out_table_fstb    # == >fstb<
    sql = "CREATE TABLE " + table_name + "(unix_time_norm INTEGER, unix_time INTEGER, zeit_norm, datum, uhrzeit, " \
                            "esp8266id STRING, software_Vs STRING, " \
                            "humidity REAL, temperature REAL, SDS_P1 REAL, SDS_P2 REAL, line_number INTEGER, " \
                            "PRIMARY KEY(unix_time_norm, esp8266id));"
    table_create(db_out, table_name, sql)

    # AB hier: (einigermaßen) normalisierte Tabellen:
    # Mess-Stationen haben Sensoren haben Messwerte
    #
    # a) Tabelle der Mess-Stationen:
    #    'stations':  > station_idx - station_name - station_Vs - station_GPS - station_comment - station_otherdata -  software_Vs)
    # b) Tabelle der Sensoren:
    #    'sensors':   > sensor_idxx - Sensor_Typ - Sensor_Name - Sensor_Vs - Sensor_Comment - sensor_otherdata<
    # c) Tabelle der Messwerte
    #     'values':  > Zeit - Mess_station_idx - sensor_idx - Messwert <
    #    Pro Zeile also immer nur _ein_ Messwert. Diese Struktur erleichtert das Hinzufügen von neuen Sensoren,
    #    von deren Werten, deren graphische Darstellung und deren Auswertung ungemein.

    ####################   JSON
    # https://stackoverflow.com/questions/2701877/sqlite-table-constraint-unique-on-multiple-columns

    table_name = mem.db_out_table_fstb_JSON  # == >fstb_JSON<
    sql = "CREATE TABLE " + table_name + \
          "(unix_time_norm INTEGER, unix_time INTEGER, datum, zeit, station_name STRING, sensordatavalues STRING, line_number INTEGER, " \
          "PRIMARY KEY(unix_time_norm, station_name)) ;"
    table_create(db_out, table_name, sql)

    # nb: 'station' == Mess-Station == zB: esp8266id;
    table_name = mem.db_out_table_stations  # == >stations<
    sql = "CREATE TABLE " + table_name + \
          "(station_idx INTEGER PRIMARY KEY, station_name STRING, station_Vs, station_GPS, station_comment, station_otherdata, " \
          "UNIQUE (station_name, station_Vs, station_GPS, station_comment, station_otherdata) );"
    table_create(db_out, table_name, sql)

    table_name = mem.db_out_table_sensors
    # nb: sensor_type == temp, humidity ... ; sensor_name = PT100, PT1000 ...
    sql = "CREATE TABLE " + table_name + \
          "(sensor_idx INTEGER PRIMARY KEY, sensor_type STRING, sensor_name STRING, sensor_Vs, sensor_comment, sensor_otherdata, " \
          "UNIQUE (sensor_type, sensor_name, sensor_Vs, sensor_comment, sensor_otherdata) );"
    table_create(db_out, table_name, sql)

    # nb: 'datum'   == yy-mm-dd hh:mm:ss  (normiert auf Messzeitpunkte)
    # nb: 'zeit'    ==          hh:mm:ss  (normiert auf Messzeitpunkte)
    table_name = mem.db_out_table_values
    sql = "CREATE TABLE " + table_name + \
          "(unix_time_norm INTEGER, datum, zeit, station_idx, station_name STRING, " \
          "sensor_idx, sensor_type STRING, value, line_number INTEGER, " \
          "PRIMARY KEY(unix_time_norm, station_idx, sensor_idx), " \
          "FOREIGN KEY (station_idx)  REFERENCES stations(station_idx) " \
          "FOREIGN KEY (sensor_idx)   REFERENCES sensors(sensor_idx) ) ;"
    table_create(db_out, table_name, sql)

    table_name = mem.db_out_table_day_station
    sql = "CREATE TABLE " + table_name + \
          "(datum, station_name STRING, UNIQUE (datum, station_name) );"
    table_create(db_out, table_name, sql)

    table_name = mem.db_out_table_stations_sensors # = 'stations_sensors'
    sql = "CREATE TABLE " + table_name + \
          "(station_name STRING, sensor_type STRING, sensor_name STRING, UNIQUE (station_name, sensor_type, sensor_name));"
    table_create(db_out, table_name, sql)


def mssge_SQL_failed(loc='', db_out = '', sql = '', row = '', ):
    mssge = 'db_out: ' + db_out + ': INSERT failed: '
    p_utils.p_terminal_mssge_note_this(mssge)

    mssge = ' SQL: >' + sql + '<'
    p_utils.p_terminal_mssge_note_this(mssge)

    mssge = ' row: >' + str(row) + '<'
    p_utils.p_terminal_mssge_note_this(mssge)

    mssge = ' loc: >' + loc + '<'
    p_utils.p_terminal_mssge_note_this(mssge)
    return


def sql_insert(cursor, sql, row, loc):
    # Besser nicht mit >with< sondern mit >try - except - finally<, weil dann eine Fehlerbehandlung möglich ist.
    # Siehe: http://zetcode.com/db/sqlitepythontutorial/  dort Abschnitt: Transactions
    try:
        cursor.execute(sql, row)
        return True
    except sqlite3.Error, e:
        # if conn:
        #     conn.rollback()
        print "\nError %s:" % e.args[0]
        mssge_SQL_failed(loc=loc, db_out = '', sql=sql, row=row)
        return False


# Die Methode der Werteübergabe an ein SELECT SQL-Statement mit Hilfe der geschweiften Klammern >{}< a lá
# >( blabla {} blublu).format('val1', 'val2')< geht nicht immer (!!): wenn bei einem SELECT nach einem
# leeren String gesucht wird, dann fügt format() auch nur einen leeren String ein: >< - und nicht >""<,
# wie es in einer SELECT-Anweisung gebraucht wird. Also wird die Methode mit >?< genommen.
#  wg. "?" placeholder:
# http://pythoncentral.io/introduction-to-sqlite-in-python/
# "SELECT name, email, phone FROM users WHERE id=?", (user_id,))

# http://sebastianraschka.com/Articles/2014_sqlite_in_python_tutorial.html
# http://zetcode.com/db/sqlitepythontutorial/
def sql_select(cursor, sql, row, loc):
    # Besser nicht mit >with< sondern mit >try - except - finally<, weil dann eine Fehlerbehandlung möglich ist.
    # Siehe: http://zetcode.com/db/sqlitepythontutorial/  dort Abschnitt: Transactions
    try:
        cursor.execute(sql, row)
        result = cursor.fetchone()
    except sqlite3.Error, e:
        print "\nError %s:" % e.args[0]
        mssge_SQL_failed(loc=loc, db_out = '', sql=sql, row=row)
    return result


def insert_row_in_table_fstb(cursor, table_fstb, row):
    # inserts one row in table >table_fstb< , zB: >fstb<
    sql = "INSERT INTO " + table_fstb + " values (?,?,?,?,?,?,?,?,?,?,?,?)"
    try:
        cursor.execute(sql, row)
        return True
    except sqlite3.Error, e:
        if conn:
            conn.rollback()
        print "\nError %s:" % e.args[0]
        loc = p_utils.p_here(level=1)
        mssge_SQL_failed(loc=loc, db_out = '', sql=sql, row=row)
        return False


def insert_data_in_table_fstb_JSON (inp_rows, db_out, table_fstb_JSON):
    # inp_rows = liste der Daten ... die in die db >db_out<, in die Tabelle >table_fstb_JSON< eingefügt werden.
    print '\ninsert_data_in_table_fstb_JSON: BEGIN '
    cnt_row = 0; cnt_insert_ok = 0 ; cnt_insert_fail = 0
    with sqlite3.connect(db_out) as conn:
        cursor = conn.cursor()
        sql = "INSERT INTO " + table_fstb_JSON + " values (?,?,?,?,?,?,?)"
        for row in inp_rows:
            cnt_row += 1    # counts line of inp_rows
            try:
                cursor.execute(sql, row)
                cnt_insert_ok += 1
                # conn.commit()
            except sqlite3.Error, e:
                cnt_insert_ok += -1
                if conn:
                    conn.rollback()
                print "\nError %s:" % e.args[0]
                loc = p_utils.p_here(level=1)
                loc += ' try -> except, cnt = ' + str(cnt_row)
                mssge_SQL_failed(loc=loc, db_out = db_out, sql=sql, row=row)
                cnt_insert_fail += 1

    # print data_file_name, row_cnt, cnt_ok , cnt_fail
    msge = '> insert_data_in_table_fstb_JSON(): insert values into database: >' + db_out + '<, table = >' + table_fstb_JSON + '<'
    print msge; p_log_this(msge)

    msge = '> insert_data_in_table_fstb_JSON(): inp_rows has ' + str(len(inp_rows)) + ' lines'
    print msge; p_log_this(msge)

    msge = '> insert_data_in_table_fstb_JSON(): row_cnt, cnt_ok, cnt_fail: ' \
           + str(cnt_row) + ' ' + str(cnt_insert_ok) + ' ' + str(cnt_insert_fail)
    print msge; p_log_this(msge)
    print 'insert_data_in_table_fstb_JSON: END \n'


def insert_station_in_table_station(cursor, db_out_table_station, station_dict, station_tuple):
    sql_select_station_idx = "SELECT station_idx FROM " + db_out_table_station + " WHERE " \
       "station_name = ? AND station_Vs = ? AND station_GPS = ? AND " \
       "station_comment = ? AND station_otherdata = ? "
    station_idx = sql_select(cursor, sql=sql_select_station_idx, row=station_tuple,
        loc='insert_station_in_table_station: SELECT station_idx')
    if (not station_idx):
        # print 'In >' + db_out_table_station + '< keine Station gefunden mit: ', station_tuple
        # Nein -> also die aktuelle Station eintragen:
        sql_insert_station = "INSERT INTO " + db_out_table_station + \
             " (station_name, station_Vs, station_GPS, station_comment, station_otherdata) " \
             "  values (?,?,?,?,?);"
        sql_insert(cursor, sql=sql_insert_station, row=station_tuple,
           loc='insert_station_in_table_station: @ insert_station')
        # Jetzt diese Station (und damit ihre >station_idx<) mittels SELECT eruieren:
        station_idx = sql_select(cursor, sql=sql_select_station_idx, row=station_tuple,
            loc='insert_station_in_table_station: SELECT station_idx')
    station_idx = station_idx[0]  # sonst Ergebnis der Form >"(1,)"<
    # In das station_dict eintragen (key = station_idx):
    station_dict.update({station_tuple: station_idx})


def insert_sensor_in_table_sensor(cursor, table_sensor, sensor_dict, sensor_tuple):
    # sensor_tuple in sensor_dict
    sql_select_sensor_idx = "SELECT sensor_idx FROM " + table_sensor + " WHERE " \
        "sensor_type = ? AND sensor_name = ? AND sensor_Vs = ? AND " \
        "sensor_comment = ? AND sensor_otherdata = ? "
    # sensor_row = sql_select(cursor, sql=sql_select_station, row=sensor_tuple, loc='insert_sensor_in_table_sensor: SELECT #1')
    sensor_idx = sql_select(cursor, sql=sql_select_sensor_idx, row=sensor_tuple,
       loc='insert_sensor_in_table_sensor: SELECT: sensor_idx')
    if (not sensor_idx):
        # print 'In >' + table_sensor + '< kein Sensor gefunden mit: ', sensor_tuple
        # Nein -> also den aktuellen Sensor eintragen:
        sql_insert_sensor = "INSERT INTO " + table_sensor + \
            " (sensor_type, sensor_name, sensor_Vs, sensor_comment, sensor_otherdata) " \
            "  values (?,?,?,?,?);"
        sql_insert(cursor, sql=sql_insert_sensor, row=sensor_tuple,
           loc='insert_sensor_in_table_sensor: @ insert_sensor_in_table_sensor')
        # Jetzt diesen Sensor (und damit dessen >sensor_idx<) mittels SELECT eruieren:
        sensor_idx = sql_select(cursor, sql=sql_select_sensor_idx, row=sensor_tuple,
           loc='insert_sensor_in_table_sensor: SELECT sensor_idx')
    sensor_idx = sensor_idx[0]  # sonst Ergebnis der Form >"(1,)"<
    # In das sensor_dict eintragen (key = sensor_idx):
    sensor_dict.update({sensor_tuple: sensor_idx})


def insert_value_in_table_value(cursor, db_out_table_value, value_tuple):
    sql_insert_value = "INSERT INTO " + db_out_table_value + \
        " (unix_time_norm, datum, zeit, station_idx, station_name, sensor_idx, sensor_type, value, line_number)" \
        "  values (?,?,?,?,?,?,?,?,?);"
    return sql_insert(cursor, sql=sql_insert_value, row=value_tuple,
       loc='insert_value_in_table_value')


def unix_timestamp_to_datetime_str(unix_timestamp):
    # https://stackoverflow.com/questions/11882718/find-last-mid-night-time-stamp-in-python
    from datetime import datetime
    import pytz  # pip install pytz

    fmt = '%Y-%m-%d %H:%M:%S'
    tz = pytz.timezone("Europe/Berlin")  # supply client's timezone here
    dt = datetime.fromtimestamp(int(unix_timestamp)).strftime(fmt)
    return dt


def get_unix_time_midnight(unix_timestamp):
    # https://stackoverflow.com/questions/11882718/find-last-mid-night-time-stamp-in-python
    from datetime import datetime
    import pytz  # pip install pytz

    fmt = '%Y-%m-%d %H:%M:%S %Z%z'
    tz = pytz.timezone("Europe/Berlin")  # supply client's timezone here

    # Get correct date for the midnight using given timezone.
    # Since we are interested only in midnight we can:

    # 1. ignore ambiguity when local time repeats itself during DST change e.g.,
    # 2012-04-01 02:30:00 EST+1100 and
    # 2012-04-01 02:30:00 EST+1000
    # otherwise we should have started with UTC time

    # 2. rely on .now(tz) to choose timezone correctly (dst/no dst)

    # moment = datetime.now(tz)
    moment = datetime.fromtimestamp(unix_timestamp, tz)

    # https://stackoverflow.com/questions/19801727/convert-datetime-to-unix-timestamp-and-convert-it-back-in-python/27914405#27914405
    # print time.mktime(moment.timetuple())

    # Get midnight in the correct timezone (taking into account DST)
    midnight = tz.localize(moment.replace(hour=0, minute=0, second=0, microsecond=0, tzinfo=None), is_dst=None)

    # Convert to UTC (no need to call `tz.normalize()` due to UTC has no DST transitions)
    dt = midnight.astimezone(pytz.utc)

    # Get POSIX timestamp
    ret_unix_timestamp = (dt - datetime(1970, 1, 1, tzinfo=pytz.utc)).total_seconds()
    # print ret_unix_timestamp
    return ret_unix_timestamp


def get_distinct_station_names(db_name, table_name='fstb'):
    # find all distinct station_name's in table: >db_name<
    sql = "SELECT DISTINCT station_name FROM '" + table_name + "' ;"
    # print sql ;  p_log_this(sql)
    station_name_table = []
    with sqlite3.connect(db_name) as conn:
        cursor = conn.execute(sql)
        for row in cursor.fetchall():
            station_name_table.append (row[0])
    return station_name_table


def db_in_get_data_by_station_name (db_name, table_name, station_name):
    # db_table_get_column_names(db_name, table_name)
    sql = "SELECT * FROM '" + table_name + "'WHERE station_name = '" + str(station_name) + "' ORDER BY unix_time ;"
    # print sql ;  p_log_this(sql)
    data_table = []
    with sqlite3.connect(db_name) as conn:
        cursor = conn.execute(sql)
        for row in cursor.fetchall():
            data_table.append(row)
    return data_table


def list_to_dict(list_in):
    # list_in  == >[{u'value_type': u'SDS_P1', u'value': u'6.45'}, ...]< == list of dictionaries
    # dict_out == > {               u'SDS_P1':           u'6.45', ...} < == dict
    # print type(list_in), list_in
    dict_out = {}
    for ele_list in list_in:
        value_type = ele_list['value_type']
        value      = ele_list['value']
        dict_out[value_type] = value
    return dict_out


def dict_to_list(dict_in):
    # dict_in  == >{"max_micro": "27558", "temperature": "24.90", "signal": "-69 dBm", ... }<
    # list_out == >[{u'value_type': u'SDS_P1', u'value': u'6.45'}, ...]<  == will be a list of dictionaries:
    list_out = []
    for k, v in dict_in.iteritems():
        dict = {}
        dict['value_type'] = k
        dict['value']      = v
        # print dict
        list_out.append(dict)
    return list_out


def find_greater_datavalues(sensordatavalues_a, sensordatavalues_b):
    # https://pymotw.com/2/json/  # https://stackoverflow.com/questions/33432421/sqlite-json1-example-for-json-extract-set
    # dict to str: json.dumps() and str to dict: json.loads()
    # [{"value_type": "SDS_P1", "value": "11.00"}, {"value_type": "SDS_P2", "value": "4.20"}, ... ]
    list_a = json.loads(sensordatavalues_a)
    list_b = json.loads(sensordatavalues_b)

    if len(list_a) >= len(list_b):
        dict_1 = list_to_dict(list_a)
        dict_2 = list_to_dict(list_b)
    else:
        dict_1 = list_to_dict(list_b)
        dict_2 = list_to_dict(list_a)

    dict_tmp = {}
    for key, val in dict_1.iteritems():
        if key in dict_2:
            val_1 = dict_1[key]
            val_2 = dict_2[key]
            num_1 = val_1
            num_2 = val_2
            if num_1 > num_2:
                dict_tmp[key] = val_1
            else:
                dict_tmp[key] = val_2
    list_res = json.dumps(dict_to_list(dict_tmp))
    return list_res


def insert_data_in_table_fstb_JSON_NORM(params):
    fstb_norm, db_out, db_out_table_fstb, table_stations, table_sensors, table_values = params

    # nb:  station_name == esp8266id
    #
    # Am Ende sollen in die Tabelle >value< die Zeilen der Messwerte in der Form:
    #     (utime_norm, station_idx, sensor_idx, value, line_nr)
    #   geschrieben werden, wobei:
    #      station_idx  (Index aus der Tabelle der Mess-Stationen >stations<)   und
    #      sensor_idx   (Index aus der Tabelle der Sensoren >sensors<)
    #   die Mess-Station und den Sensor eindeutig identifizieren.

    # station_dict = (python) dict of: {station_idx: [station_name, station_Vs, station_GPS, station_comment, station_otherdata}
    # sensor_dict  = (python) dict of: {sensor_idx:  [sensor_type, sensor_name, sensor_Vs, sensor_comment, sensor_otherdata]}
    # value_dict   = (python) dict of: {sensor_idx:  [station_idx, sensor_idx, value, line_number]}
    #
    # for every inp_row in sensordatavalues:
    #     extract from inp_row: station_tuple == tuple (station_name, station_Vs, station_GPS, station_comment, station_otherdata)
    #     if station_tuple is not in table stations:
    #        insert station_tuple in table stations   ! stations_id will be created
    #     get station_idx from table stations
    #
    #     extract from inp_row: sensor_tuple  == tuple (sensor_type, sensor_name, sensor_Vs, sensor_comment, sensor_otherdata)
    #     if sensor_tuple is not in table sensors_set:
    #        insert sensor_tuple in table sensors_set     ! sensor_idx will be created
    #     get sensor_idx from table sensors_set
    #
    #     extract from inp_row: value_tuple (utime_norm, value)
    #
    #     compose from inp_row and sensor_idx: (utime_norm, station_idx, sensor_idx, value, line_nr)
    #

    print 'insert_data_in_table_fstb_JSON_NORM: BEGIN '

    station_dict = {}
    sensor_dict  = {}
    value_dict   = {}
    cnt_row = 0;
    cnt_insert_ok = 0       ; cnt_insert_fail = 0          # cnt succesfull INSERT into >fstb<
    cnt_insert_value_ok = 0 ; cnt_insert_value_fail = 0   # cnt succesfull INSERT into >"values"<

    with sqlite3.connect(db_out) as conn:
        # https://stackoverflow.com/questions/33658216/insert-null-into-sqlite-from-python
        # https://stackoverflow.com/questions/26038935/how-to-insert-null-in-table-with-python-sqlite3
        # test = True
        cursor = conn.cursor()

        station_dict = {}
        sensor_dict = {}

        # new_row = unix_time_NORM, TMP_unix_time, station_name, sensordatavalues, TMP_line_nr
        # sensordatavalues =
        # [{"value_type": "SDS_P1", "value": "13.23"}, {"value_type": "SDS_P2", "value": "9.50"},
        #  {"value_type": "temperature", "value": "24.90"}, {"value_type": "humidity", "value": "51.80"},
        #  {"value_type": "samples", "value": "758900"}, {"value_type": "min_micro", "value": "189"},
        #  {"value_type": "max_micro", "value": "27564"}, {"value_type": "signal", "value": "-70"}]

        station_Vs = '1'           # per definitionem!
        for row in fstb_norm:
            unix_time_norm, unix_time, yymmdd_hhmmss, hhmmss, station_name, sensordatavalues, line_nr = row

            # Ist die Station schon in der sql-Tabelle >station<?
            station_GPS, station_comment, station_otherdata = '', '', ''
            station_tuple = station_name, station_Vs, station_GPS, station_comment, station_otherdata

            # Wir wollen nicht bei jedem Messwert in der Datenbank schauen, ob die zugehörige Mess-Station
            # schon in der Datenbank vorhanden ist - das geht schneller über ein python Dictionary:
            # Im dictionary >station_dict< sind {key: value} Paare der Form: {station_tuple: station_idx} -
            # so kann man im Dictionary nach dem Index der Mess-Station schauen - wenn vorhanden.
            #
            # In der aktuellen Tabellenzeile >row< liegen auch die Daten vor, die die Mess-Station identifizieren.
            # Diese Identifikations-Daten landen im Tuple: >station_tuple<.
            # Dann wird geschaut ob dafür schon ein Schlüssel >station_tuple< im Dict >station_dict< existiert.
            #  Falls ja, wird dieser verwendet, um >station_idx< zu bekommen.
            #  Falls nicht, muss die >station_idx< in der Datenbank gesucht werden.
            #    Falls dort (in der Datenbank ) noch kein Eintrag für eine entsprechende Mess-Station existiert,
            #    wird die Mess-Station in die Datenbank eingetragen. Danach wird die station_idx aus der Datenbank abgefragt;
            #    dann wird das {key: value} == {station_tuple: station_idx} in das Dict >station_dict< eingetragen,
            #    und schließlich über den key = station_tuple aus dem Dict, die station_idx abgefragt.
            # Jetzt haben wir: >station_idx<
            #
            if not station_tuple in station_dict:
                # Frage die Datenbank nach der station_idx:
                insert_station_in_table_station(cursor, table_stations, station_dict, station_tuple)
                conn.commit()

            station_idx = station_dict[station_tuple]
            # >station_idx< ist jetzt bekannt

            # print 'insert_data_in_table_fstb_JSON_NORM: station_idx >' + str(station_idx) + '<'

            # sensordatavalues == JSON list of dict.
            # Jetzt laufen wir durch alle Paare: >sensor_name: value< in sensordatavalues.


            sensor_Vs, sensor_comment, sensor_otherdata = '', '', ''

            if sensordatavalues == '':
                # Ein wg plotly eingefügter Zeitpunkt ohne Messwerte:
                # Für alle Sensoren in >sensor_dict< wird der Messwert None eingefügt,
                # denn ploty macht daraus dann eine Lücke.
                sens_vals_dict = {}
                if sensor_dict:
                    for sensor_tuple in sensor_dict:
                        sensor_type, sensor_name, sensor_Vs, sensor_comment, sensor_otherdata = sensor_tuple
                        sens_vals_dict[sensor_type] = None
                else:
                    print sensor_dict
                    print sens_vals_dict
                # exit()
            else:
                sens_vals_dict = list_to_dict(json.loads(sensordatavalues))

            val_humidity = None
            val_temperature = None
            val_SDS_P1 = None
            val_SDS_P2 = None

            for sensor_name, value in sens_vals_dict.iteritems():
                if   sensor_name == "humidity":
                    sensor_type, sensor_name = "humidity", "DHT22"
                    val_humidity = value
                elif sensor_name == "temperature":
                    sensor_type, sensor_name = "temperature", "DHT22"
                    val_temperature = value
                elif sensor_name == "SDS_P1":
                    sensor_type, sensor_name = "SDS_P1", "SDS011"
                    val_SDS_P1 = value
                elif sensor_name == "SDS_P2":
                    sensor_type, sensor_name = "SDS_P2", "SDS011"
                    val_SDS_P2 = value
                else:
                    sensor_type, sensor_name = sensor_name, sensor_name

                sensor_tuple = sensor_type, sensor_name, sensor_Vs, sensor_comment, sensor_otherdata

                # Ist der Sensor >sensor_idx< schon bekannt, dh in >sensor_dict<?
                # Nein?
                #   -> In db-Tabelle >sensors<?
                #   Nein?
                #     -> Trage ihn ein.
                #     -> Frage >sensor_idx< aus der db ab.
                #     -> Trage >sensor_idx< in >sensor_dict< ein
                # Hole >sensor_idx< aus >sensor_dict<

                if not sensor_tuple in sensor_dict:
                    # Frage die Datenbank nach der sensor_idx:
                    insert_sensor_in_table_sensor(cursor, table_sensors, sensor_dict, sensor_tuple)
                    conn.commit()

                sensor_idx = sensor_dict[sensor_tuple]
                # >sensor_idx< ist jetzt auch bekannt
                # (unix_time_norm INTEGER, datum, zeit, station_idx, station_name, sensor_idx, sensor_type, value, line_number INTEGER, " \
                utime_value_tuple = unix_time_norm, yymmdd_hhmmss, hhmmss, station_idx, station_name, sensor_idx, sensor_type, value, line_nr
                # print cnt_row, utime_value_tuple
                if insert_value_in_table_value(cursor, table_values, utime_value_tuple):
                    cnt_insert_value_ok += 1
                else:
                    cnt_insert_value_fail += 1
                # unix_time_norm, unix_time, zeit_norm, datum, uhrzeit, esp8266id, software_Vs, humidity, temperature, SDS_P1, SDS_P2,line_number

            value_tuple = unix_time_norm, '', yymmdd_hhmmss, yymmdd_hhmmss, hhmmss, station_name, '', \
                          val_humidity, val_temperature, val_SDS_P1, val_SDS_P2, line_nr

            # HIER endlich die Daten in die Datenbank-Tabelle db_out_table_fstb eintragen:
            if insert_row_in_table_fstb(cursor, db_out_table_fstb, value_tuple):
                cnt_insert_ok += 1
            else:
                cnt_insert_fail += 1

            cnt_row += 1    # counts line of fstb_norm

    # print data_file_name, cnt, cnt_ok , cnt_fail
    # msge = '> insert_data_in_table_fstb_JSON_NORM(): insert values from: >' + table_value + '< to database: >' + db_out + '<.'
    msge = '> insert_data_in_table_fstb_JSON_NORM(): insert values into database: >' + db_out + '<, table = >' + db_out_table_fstb + '<'
    print msge; p_log_this(msge)

    msge = '> insert_data_in_table_fstb_JSON_NORM(): fstb_norm has ' + str(len(fstb_norm)) + ' lines'
    print msge; p_log_this(msge)

    msge = '> insert_data_in_table_fstb_JSON_NORM(): INSERT lines in >fstb<: cnt, cnt_ok, cnt_fail = ' \
           + str(cnt_row) + ' ' + str(cnt_insert_ok) + ' ' + str(cnt_insert_fail)
    print msge; p_log_this(msge)

    msge = '> insert_data_in_table_fstb_JSON_NORM(): INSERT lines in >values<: cnt, cnt_ok, cnt_fail = ' \
           + str(cnt_row) + ' ' + str(cnt_insert_value_ok) + ' ' + str(cnt_insert_value_fail)
    print msge; p_log_this(msge)
    print 'insert_data_in_table_fstb_JSON_NORM: END \n'


def make_instants_table():
    # make table: list of >instant< with instant == [cnt, secs_since_midnight, False]
    # cnt = 0..575 ; secs = 0 + cnt * delta_t_mess ; True if there is data for this moment.

    # https://stackoverflow.com/questions/7594656/how-to-determine-when-dst-starts-or-ends-in-a-specific-location-in-python
    day_secs     = 60 * 60 * 25  # == Anzahl Sek/Tag  # * 25 (!) wg Zeitumstellung! == DST ==  day ligth saving
    secs_since_midnight = 0
    idx_instants = []
    for cnt in range(0, day_secs // delta_t_mess):
        secs_since_midnight = secs_since_midnight + delta_t_mess
        instant = [cnt, secs_since_midnight, False]
        idx_instants.append(instant)
        # [0, 150, False]       <=> default: keine Messwert vorhanden für Zeitpunkt 0
        # [1, 300, False]       <=> default: keine Messwert vorhanden für Zeitpunkt 1
        # [2, 450, False]       <=> default: keine Messwert vorhanden für Zeitpunkt 2
        # ...
        # [575, 86400, False]   <=> default: keine Messwert vorhanden für Zeitpunkt 575   (letzter Zeitpunkt des Tages)
    return idx_instants


def add_day_time_to_fstb_NORM(fstb_norm):
    fstb_norm_out = []
    for row in fstb_norm:
        unix_time_NORM, TMP_unix_time, station_name, sensordatavalues, TMP_line_nr = row
        yymmdd_hhmmss = unix_timestamp_to_datetime_str(unix_time_NORM)
        # yymmdd        = unix_timestamp_to_datetime_str(unix_time_NORM)[:10]
        hhmmss        = unix_timestamp_to_datetime_str(unix_time_NORM)[11:]
        new_row = unix_time_NORM, TMP_unix_time, yymmdd_hhmmss, hhmmss, station_name, sensordatavalues, TMP_line_nr
        fstb_norm_out.append(new_row)
    return fstb_norm_out


def insert_distinct_day_stations(db_out, table_values = mem.db_out_table_values, table_day_station = mem.db_out_table_day_station):
    # find all distinct days with their distinct station_name-s in table: >table_values<
    sql = "SELECT DISTINCT date(datum), station_name FROM " + table_values + " ORDER BY datum, station_name ;"
    # print db_out, sql ;  p_log_this(sql)
    rslt_table = []
    with sqlite3.connect(db_out) as conn:
        cursor = conn.cursor()
        try:
            cursor.execute(sql, '') # '' <=> no values for parameter substitution.
            rslt_table = cursor.fetchall()
        except sqlite3.Error, e:
            print "\nError %s:" % e.args[0]
            mssge_SQL_failed("insert_distinct_day_stations", db_out = db_out, sql=sql, row=row)

    if rslt_table:
        # print '#1 ', type(rslt_table), len(rslt_table), rslt_table
        with sqlite3.connect(db_out) as conn:
            cursor = conn.cursor()
            sql = "INSERT INTO " + table_day_station + " values (?,?);"
            for idx, row in enumerate(rslt_table):
                try:
                    cursor.execute(sql, row)
                except sqlite3.Error, e:
                    print "\nError %s:" % e.args[0]
                    mssge_SQL_failed("insert_distinct_day_stations", db_out = db_out, sql=sql, row=row)


def normalize_and_save_values (db_in, db_in_table_name, db_out, db_out_table_fstb_JSON):
    # Drei Aufgaben:
    #
    # 0. Hole Daten aus: db >db_in<, Tabelle: >db_in_table_fstb<
    # 1. 'Normalisiere' die Messzeitpunkte:
    #    die Staubwerte werden von den Sensoren zu unterschiedlichen Zeitpunkten gemessen,
    #    wobei die Intervalle (delta_t_mess) identisch sind. Um eine Systematik hineinzubringen,
    #    werden die Zeitpunkte jeweils leicht verschoben, maximal um (delta_t/2), wobei der erste
    #    Zeitpunkt 00:00 ist, d.h. Mitternacht. Danach werden jeweils nach 150 Sekunden (ideales delta_t)
    #    die weiteren Messzeitpunkte festgelegt werden.
    #    Da das reale delta_t == 147 Sekunden ist, muss nach etwa 50  - 70 realen Messzeitpunkten
    #    (mit zugehörigen Messwerten) einer getilgt werden (samt zugehörigen Messwerten), da zwei reale
    #    Messzeitpunkte, auf einen idealen fallen. (Als Messwerte, die erhalten werden, wird von den beiden
    #    die zur Verfügung stehen, jeweils immer der größere ausgewählt - was ein wenig umständlich zu codieren ist.)
    # 2. Füge NULL (None, NaN) Werte ein, für alle Messzeitpunkte für die keine Messwert vorliegt.
    #    So wird vermieden, dass plotly auseinanderliegende Werte miteinander verbindet, statt
    #    eine Lücke einzufügen. Auch kann man in R die Werte dadurch besser darstellen.
    #    Auch dies ist etwas umständlich.
    #
    # 3.a Speichere die Daten in db >db_out<, Tabelle:  >db_out_table_fstb<
    # 3.b Speichere die Daten in db >db_out<, Tabellen: >db_out_table_value< und >db_out_table_sensor<

    p_log_this('begin')

    station_name_table = get_distinct_station_names(db_in, db_in_table_name)

    data_org   = []   # original Messwerte mit den nicht normierten Messzeitpunkten
    fstb_norm  = []   #          Messwerte mit den       normierten Messzeitpunkten  (für JSON-Tabellen, einigermaßen normalisiert)

    # Welche Stationen (zB esp8266's) haben Daten gemessen?

    # idx_instants = Index-Liste
    #    index 0 .. 575
    #    'normierte' Zeitpunkte in Sekunden (alle 150 Sekunden) von 00:00 bis 24:00
    #    False/True: True wenn für diesen Zeitpunkt ein Messwert vorliegt.

    idx_instants   = []  # Liste der normierten Zeitpunkte vorbereiten und vorbelegen:
    idx_instants = make_instants_table()

    # Jetzt die Daten für die vorhandenen Sensoren (i.e. station_name) bearbeiten:
    # Für jeden esp8266:
    #   Für jeden Messwert:
    #     Falls neuer Tag:
    #       Falls Vortag vorhanden:
    #          Für alle (inzwischen normalisierten) Zeitpunkte des Vortags (in python-Liste >fstb_norm<)
    #              Wenn laut index.Liste kein Messwert vorhanden:
    #                => Messwerte = None.
    #               [damit plotly nicht weit auseinanderliegenden Messzeitpunkte und
    #                deren -werte mit Linien verbindet und statt dessen Lücken lässt:
    #                https://plot.ly/python/line-charts/    ( => 'connectgaps')]
    #       Regeneriere Index-Liste
    #   Normalisiere Messzeitpunkt (i.e. verschiebe ihn nach vorn)
    #   Falls für Zeitpunkt schon Werte vorhanden:
    #     suche jeweils größeren Wert
    #   Trage Werte in Ergebnisliste >fstb_norm< ein.
    # Speichere >fstb_norm< in SQL-db
    #

    for station_name in station_name_table:  # also für jede Mess-Station
        if data_org:      del data_org[:]
        if fstb_norm:     del fstb_norm[:]

        cnt_miss_vals     = 0    # Fehlende Messzeitpunkte in der Zeitreihe eines Tages
        cnt_lines_dropped = 0    #
        datum_alt         = None # Es gibt keinen Vortag, dh die Schleife wird zum ersten Mal durchlaufen.
        midnight          = 0    # Mitternacht == 0 Sekunden
        idx_old           = -1   # idx == index des normalisierten Messzeitpunkts in der Liste >idx_instants[]<

        # Pro Mess-Station: alle Tage, alle Sensoren
        data_org = db_in_get_data_by_station_name(db_in, db_in_table_name, station_name)
        # unix_time, station_name, sensordatavalues, line_JSON, line_nr = row

        # Normalisiere Messzeitpunkte: verschiebe realen auf idealen Messzeitpunkt.
        # Leider etwas umständliche Rechnerei (Geht es nicht doch einfacher??):
        # In der Liste >idx_instants[]< sind Listen der Form:
        #  [cnt, secs_since_midnight, ..., used]
        #   cnt                 == ein Zähler
        #   secs_since_midnight == Sekunden seit Mitternacht
        #   used                == False -> True falls dieser Messzeitpunkt verwendet wird.
        #
        # Die Liste >idx_instants[]< hat genau ((60 * 60 * 24) / 150) = 576 Zeilen, und
        # >secs_since_midnight< wächst pro Zeile um 150.
        #
        # a) Für jeden realen Messzeitpunkt wird der nächst kleinere ideale Messzeitpunkt in der
        # Liste gesucht und im neuen Eintrag ersetzt.
        # b) Falls der ideale Messzeitpunkt verwendet wird, wird >used< auf True gesetzt.
        #
        for row in data_org:  # orig. Messzeitpunkt + Messwerte
            # nb >line_JSON< == original line from '.log file will be skipped in fstb_JSON_NORM !
            unix_time, station_name, sensordatavalues, line_JSON, line_nr = row
            date_time = unix_timestamp_to_datetime_str(unix_time)[:10]
            datum     = date_time [:10]

            if datum_alt is None:
                datum_alt = datum
                midnight  = get_unix_time_midnight(unix_time)
                # print datum + '<'

            if datum_alt != datum:   # Tag hat gewechselt ->

                # 1) Jetzt zuerst die Zeitreihen des Vortags (so vorhanden) bearbeiten/abspeichern:
                #    Wg plotly: füge für Sequenzen von Zeitpunkten ohne Messwerte den Messwert >None< ein
                #    Finde die Leerstellen der Zeitreihe des Vortags, i.e.: idx_instants[i][4] == False
                # 2) Nä Tag vorbereiten: u.a. neue unix_time für Mitternacht

                unix_time_NORM_0 = 0
                unix_time_NORM_1 = 0
                for i, instant in enumerate(idx_instants):
                    # gehe alle Zeitpunkte des Vortags durch
                    if not instant[2]:    # i.e.; (instant[2] == False) <=> Kein Messwert für diesen Zeitpunkt.
                        # Merke diesen als >unix_time_NORM_0< oder als >unix_time_NORM_1<
                        cnt_miss_vals += 1
                        unix_time_NORM = int(midnight + instant[1])
                        # liegt vorher ein leerer Zeitpunkt?
                        if not unix_time_NORM_0:
                            # Nein, also erster leerer Zeitpunkt
                            unix_time_NORM_0 = unix_time_NORM
                        else:
                            # Ja, also also schon leerer Zeitpunkt davor ->
                            # der aktuelle wird als vorläufig letzter einer Serie gespeichert.
                            # if not unix_time_NORM_1:
                            #     unix_time_NORM_1 = unix_time_NORM
                            # 1495058400
                            unix_time_NORM_1 = unix_time_NORM
                    else: # i.e.; (instant[2] == False) <=>
                        # => Es gibt jetzt einen Messwert für diesen aktuellen Zeitpunkt
                        if unix_time_NORM_0 and unix_time_NORM_1:
                            # Es gab vorher eine Folge von mindestens 2 leeren Zeitpunkten: speichere sie:
                            new_row = unix_time_NORM_0, None, station_name, '', ''
                            fstb_norm.append(new_row)
                            new_row = unix_time_NORM_1, None, station_name, '', ''
                            fstb_norm.append(new_row)
                            unix_time_NORM_0 = 0
                            unix_time_NORM_1 = 0
                        else:
                            # Es gab nur einen einzigen leeren Zeitpunkt -> speichere ihn:
                            if unix_time_NORM_0:
                                new_row = unix_time_NORM_0, None, station_name, '', ''
                                fstb_norm.append(new_row)
                                unix_time_NORM_0 = 0
                                unix_time_NORM_1 = 0
                if cnt_miss_vals:  # cnt_miss_vals == Fehlende Messzeitpunkte in der Zeitreihe eines Tages
                    # print 'normalize_and_save_values:  Datum:', datum_alt, 'station_name:',station_name, '# fehlende Werte:', cnt_miss_vals
                    print 'Mess-Station:', station_name, ' # ', datum_alt, ' fehlende Werte:', cnt_miss_vals

                # Jetzt - nachdem die Vortagsliste verarbeitet wurde - die Liste >idx_instants[]< für den aktuellen Tag vorbereiten:
                cnt_miss_vals = 0  # zähle fehlende Messzeitpunkte in den Originaldaten
                for i, instant in enumerate(idx_instants):
                    instant[2] = False

                datum_alt = datum
                # Mitternacht des neuen Tages als unix_time:
                midnight = get_unix_time_midnight(unix_time)
                # print midnight
                idx_old = -1

            seconds = unix_time - midnight  # seconds since midnight
            idx     = int((seconds // delta_t_mess))
            # Jetzt werden alle Messzeitpkte um max >delta_t_mess< Sekunden nach vorne verlegt:

            # try:
            #     new_instant = idx_instants[idx][1]
            # except:
            #     act_date_time     = unix_timestamp_to_datetime_str(unix_time)
            #     act_date_midnight = get_unix_time_midnight(unix_time)
            #     print '>' + datum + '<', date_time, act_date_time, idx, seconds, unix_time, midnight, act_date_midnight
            #     sys.stderr.flush()

            # 1. Markiere Zeitpunkt:
            new_instant = idx_instants[idx][1]
            idx_instants[idx][2] = True   # <=> Für idx_instants[idx] liegt Messwert vor.
            # unix_time_NORM  == ist der neue 'normierte' Messzeitpunkt als unix_timestamp
            # zeit_NORM     == ist der neue 'normierte' Messzeitpunkt als python - datetime Objekt
            unix_time_NORM = int(midnight + new_instant)

            # Kann sein, dass für den virtuellen Messzeitpunkt (mit Index idx)
            # schon ein Wert vorliegt. Das kommt ca 8 - 9 mal /die vor.
            # Der zweite Messwert überschreibt dann den ersten. Eigentlich sollte
            # aber der jeweils größere Messwert erhalten bleiben...
            # (Aber was, wenn der SDS_P1 größer und der SDS_P2 kleiner ist?
            # -> jeweils den größeren behalten?) Ja genau.

            if idx == idx_old:
                # print idx, idx_old, fstb_norm[-1]
                TMP_unix_time_NORM, TMP_unix_time, TMP_station_name, sensordatavalues_0, TMP_line_nr = fstb_norm.pop()
                # del fstb_norm[-1]
                cnt_lines_dropped += 1
                # TMP_unix_time_NORM, TMP_unix_time, TMP_esp8266id, sensordatavalues_0, TMP_line_nr = fstb_norm[-1]
                delta_t = unix_time - TMP_unix_time
                # print '\n', cnt_lines_dropped, '1:', idx, TMP_unix_time_NORM, TMP_unix_time, delta_t, station_name, TMP_line_nr, type(sensordatavalues_0), type(sensordatavalues)
                # print       cnt_lines_dropped, '2:', idx,     unix_time_NORM,     unix_time, delta_t, station_name,     line_nr, type(sensordatavalues_0), type(sensordatavalues)
                sensordatavalues = find_greater_datavalues(sensordatavalues_0, sensordatavalues)
                new_row = unix_time_NORM, TMP_unix_time, station_name, sensordatavalues, TMP_line_nr
            else:
                new_row = unix_time_NORM, unix_time    , station_name, sensordatavalues, line_nr
                # new_row = unix_time_NORM, station_name, , line_nr

            # In der Zeile >new_row< steht nun normierter Zeitpunkt und alle zugehörigen Messwerte aller Sensoren.
            # Diese Zeile kommt in die python-Liste >fstb_norm<  (die nachher in die SQL-Tabelle >fstb< übernommen wird.

            # if new_row[6] == None:
            #     p_utils.p_terminal_mssge_error(mssge='Error')
            #     sys.exit()

            fstb_norm.append(new_row) # >fstb_norm< ist python-Liste     

            idx_old = idx

        # Pro esp8266 finden sich in >fstb_norm< nun für
        #  die vorhandenen Zeitpunkte alle erhobenen Messwerte mit normiertem Zeitpunkt.
        print '\nnormalize_and_save_values: '
        print 'station_name:   ', station_name
        print 'len(data_org):  ', len(data_org),   'lines '
        print 'len(fstb_norm): ', len(fstb_norm),  'lines; lines_dropped: ', cnt_lines_dropped

        fstb_norm = add_day_time_to_fstb_NORM(fstb_norm)

        # Speichere alle Daten ... als JSON - String
        insert_data_in_table_fstb_JSON (fstb_norm, db_out, db_out_table_fstb_JSON)

        # Speichere Daten ...
        #   Messdaten in der Tabelle >values<,
        #   Mess-Stationen in der Tabelle >stations<,
        #   Sensoren in der Tabelle >sensors<:
        params = fstb_norm, db_out, \
                 mem.db_out_table_fstb, mem.db_out_table_stations, mem.db_out_table_sensors, mem.db_out_table_values
        insert_data_in_table_fstb_JSON_NORM (params)
        p_log_this('end')


def make_fstb_pivot(db_out, db_out_table_values, fstb_pivot):
    """
    'fstb_pivot' == pivot Tabelle wird aus der db-Tabelle >values< erzeugt:
    im Grunde wird die fstb-Tabelle wieder rückwärts aus der Tabelle >values<
    zusammengesetzt, (Zeit ... - Spalte_1 ... Spalte_n), wobei die
    Spaltennamen von Spalte_1 ... Spalte_n die Namen der Sensoren sind.
    Was erstmal sinnlos erscheint ... aber: falls eine neue Mess-Stetion
    dazukommt oder neue Sensoren, so wird die gesamte Tabelle >fstb_pivot<
    ersetzt und eine neue Spalte für den neuen Sensor kommt dazu. Und das
    macht Sinn, weil die Programmierung von plot.ly, d.h. die Art, wie sich
    plot.ly Werte holen dadurch relativ einfach bleibt. Hoffentlich.
    """
    print 'make_fstb_pivot: BEGIN '
    p_log_this('Begin')

    # Die interessierenden Spalten aus >db_out_table_values< selektieren:
    sql = "SELECT unix_time_norm, datum, zeit, station_name, sensor_type, value FROM "\
          + db_out_table_values + " ORDER BY unix_time_norm ;"
    # print sql ;  p_log_this(sql)
    conn = sqlite3.connect(db_out)
    # in ein pandas dataframe umwandeln (denn pandas kann daraus eine pivot-Tabelle machen)
    df = pd.read_sql_query(sql, conn)

    # pd.options.display.max_columns = 10
    # with pd.option_context('display.max_rows', 8, 'display.width', 100):
    #     print(df)

    # Jetzt die pivot-Tabelle erzeugen. War mühselig herauszubekommen wie das geht!
    # Do not know why, really not. But it works for me.
    # The key was: >aggfunc='first'< that solved the issue:
    #   'pandas.core.base.DataError: No numeric types to aggregate'
    df = pd.pivot_table(df, index=['unix_time_norm', 'datum', 'zeit', 'station_name'], columns='sensor_type', values='value', aggfunc='first')

    # with pd.option_context('display.max_rows', 50, 'display.width', 150):
    #     print(df)

    # insert data into table >fstb_pivot<:
    # Mit Hilfe von pandas das Ergebnis in einem Rutsch in eine neue Tabelle
    # (mit korrekten Spalten und Spaltennamen) abspeichern:
    df.to_sql(fstb_pivot, conn, if_exists='replace')
    conn.close()
    p_log_this('end')
    print 'make_fstb_pivot: END '


def insert_distinct_stations_sensors(db_out, table_values, stations_sensors = mem.db_out_table_stations_sensors):
    # find all distinct combinations: station_name, sensor_name in table: >values<

    # sql = 'SELECT DISTINCT station_name, sensors.sensor_type, sensors.sensor_name FROM "values" ' \
    #             'INNER JOIN sensors ON "values".sensor_idx = sensors.sensor_idx ' \
    #             'ORDER BY station_name, sensors.sensor_name  ;'

    sql = 'SELECT DISTINCT station_name, sensors.sensor_type, sensors.sensor_name FROM {} ' \
                'INNER JOIN sensors ON {}.sensor_idx = sensors.sensor_idx ' \
                'ORDER BY station_name, sensors.sensor_name  ;'.format(table_values, table_values)

    print db_out, sql ;  p_log_this(sql)
    with sqlite3.connect(db_out) as conn:
        cursor = conn.cursor()
        try:
            cursor.execute(sql, '') # '' <=> no values for parameter substitution.
            tmp_table = cursor.fetchall()
        except sqlite3.Error, e:
            print "\nError %s:" % e.args[0]
            mssge_SQL_failed(p_utils.p_here('', 2), db_out = db_out, sql=sql, row=row)

    rslt_table = sorted(tmp_table, key=lambda k: k[0])
    if rslt_table:
        # for p in rslt_table: print p
        with sqlite3.connect(db_out) as conn:
            cursor = conn.cursor()
            sql = "INSERT INTO " + stations_sensors + " values (?,?,?);"
            for idx, row in enumerate(rslt_table):
                # print idx, row
                try:
                    cursor.execute(sql, row)
                except sqlite3.Error, e:
                    print "\nError %s:" % e.args[0]
                    mssge_SQL_failed(p_utils.p_here('', 2), db_out = db_out, sql=sql, row=row)


def main():
    p_log_this('begin')
    # eval_confargs()


    # globale Variablen in class.mem
    db_JSON_fn = os.path.normpath(confargs.db_JSON_fn)
    db_JSON_dir = os.path.normpath(confargs.db_JSON_dir)
    db_JSON_path = os.path.normpath(os.path.join(db_JSON_dir, db_JSON_fn))
    # db_in = r'C:\tmp\sqlite\feinstaub_0011.db'
    mem.db_in = db_JSON_path
    msge = 'Input:  db_in:  ' + mem.db_in
    print msge;  p_log_this(msge)

    db_norm_fn = os.path.normpath(confargs.db_norm_fn)
    db_norm_dir = os.path.normpath(confargs.db_norm_dir)
    db_norm_path = os.path.normpath(os.path.join(db_norm_dir, db_norm_fn))
    # db_out = r'C:\tmp\sqlite\feinstaub_0011_NORM.db'
    mem.db_out = db_norm_path
    msge = 'Output: db_out: ' + mem.db_out
    print msge;  p_log_this(msge)

    # dir which contains JSON files
    feinstaub_dir = os.path.normpath(confargs.files_JSON_dir)
    msge = 'feinstaub_dir: >' + feinstaub_dir + '<'
    print msge;  p_log_this(msge)

    # (make_new_db = True) => Alte Datenbank wird gelöscht und neue db frisch angelegt
    fstb_dta_to_db_mod.make_sqlite_db(fn_db = db_JSON_path, make_new_db = True)
    #fstb_dta_to_db_mod.make_sqlite_db(fn_db = db_JSON_path, make_new_db = False)
    fstb_dta_to_db_mod.process_all_json_data_files(feinstaub_dir = feinstaub_dir, fn_db = db_JSON_path)

    make_new_db = True
    # make_new_db = False
    if make_new_db:
        p_utils.p_file_delete(mem.db_out)

    if not os.path.exists(mem.db_out) or make_new_db :
        msge = 'database: >' + mem.db_out + '< does not exist.'
        print msge;
        p_log_this(msge)
        db_out_make(db_out= mem.db_out)
        db_out_tables_create(db_out= mem.db_out)

    # Normalisiere die Zeiten; speichere das Ergebnis in unterschiedlichem Format in unterschiedlichen Tabellen,
    # nämlich als JSON-String sowie in den Tabellen >values<, >stations< und >sensors<:
    normalize_and_save_values    (mem.db_in , mem.db_in_table_fstb_JSON, mem.db_out, mem.db_out_table_fstb_JSON)
    # erzeuge aus >values< Tabelle die signalisiert, an welchem Tag welche Station Messwerte geliefert hat.
    insert_distinct_day_stations (mem.db_out, mem.db_out_table_values  , mem.db_out_table_day_station)
    # mache aus >sensors< pivot Tabelle mit Sensortypen als Spaltennamen:
    make_fstb_pivot(mem.db_out, mem.db_out_table_values, mem.db_out_table_fstb_pivot)
    # erzeuge aus >values< Tabelle die angibt, über welche Sensoren welche Messstation verfügt:
    insert_distinct_stations_sensors(mem.db_out, mem.db_out_table_values, mem.db_out_table_stations_sensors)

    p_log_this('end')


if __name__ == "__main__":
    # https://askubuntu.com/questions/656771/process-niceness-vs-priority
    p_utils.p_program_name_and_dir_print()
    p_log_init(log_dir = 'log', log_fn = r'feinstaub_data_normalize.log')
    p_log_start()

    # read commandline arguments:
    x_CAParser.x_parser()

    # optional reading of cfg-file: (r' == raw string) 
    # x_CAParser.x_parser('--conf-file', r'.\cfg\feinstaub_data_normalize_02.cfg')

    main()

    p_log_end()
    p_utils.p_terminal_mssge_success()
    # p_utils.p_terminal_mssge_note_this()
    # p_utils.p_terminal_mssge_error()
    p_utils.p_exit()

    # You may use >pyinstaller.exe< to dist your program:
    # pyinstaller.exe --onefile feinstaub_data_normalize_02.py

# # 2017_07_27